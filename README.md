
# Advanced System Lab - Team 04

# t-Distributed Stochastic Neighbor Embedding

## Table of Contents

[[_TOC_]]

## Project Desciption

This is the code base for the advanced system lab semester project. In this project, we are trying to accelerate the classic dimension reduction algorithm, t-SNE, for a single core of CPU. t-SNE is a complicated algorithm and bears many lines of code. In this project, we are trying to indentify what the time-consuming kernels of t-SNE are and focus on improving these kernels.

## Project Structure

`bhtsne/`: A C++ implementation from Laurens van der Maaten. Found [here](https://lvdmaaten.github.io/tsne/).

`common/`: Utility functions used project wide, e.g., a timing construct.

`kernels/`: Numerical kernels. More details later.

`test/`: Test files for kernels. More details later.

`main.cpp`: The real t-SNE program.

`parameters.hpp`: The constants and hyper-parameters defined for the t-SNE program.

`utility.hpp`: Utility functions for the t-SNE program, e.g., loading and saving data.

`visualize.py`: visualization of the final result.

## Requirement

- C++ 14
- python 3.5+
- x86 platform (the performance counter is not portable)

## Available kernels

| Kernel      | Explanation                                                  |
| ----------- | ------------------------------------------------------------ |
| computegp   | Compute variance. Before the training phase, find for each data point the best variance based on perplexity. |
| updgradient | Compute the gradient and update the output in each training iteration. |
| zeromean    | Centralize the data. For a N*D matrix X, zero out the mean for each dimension. |
| computesed  | Compute pairwise Euclidean distances and write to a N*N matrix.|

## Usage

### Work with a kernel

If you want to work on improving a kernel function, first navigate to the `kernels/` directory and choose which one you want to modify. Each file corresponds to a kernel indicated by the file name. When writing a new version of a kernel, first define a namespace with a new version number and begin your implementation in this namespace.

```cpp
namespace zeroMeanv1 {
    void zeroMean(double* X, int N, int D) { /* Impl */ }
} // version 1

namespace zeroMeanv2 {
    void zeroMean(double* X, int N, int D) { /* better Impl */}
} // version 2
```

After you finish working on improving a new version of certain kernel, you always want to test if it is correct and if the performance has been improved. To do this, first make sure to change the namespace in `test/test.cpp`.

```cpp
// using namespace zeroMeanv1 // make sure to comment out the old version
using namespace zerMean v2 // apply your optimized version
```

After you have finished above, compile it.

```shell
# assume you are in the source root directory
make kernel_test # this will do an out-of-source build
./build/kernel_test <your kernel name>
```

Kernels that are test-able for now are `computegp`, `updgradient`, `zeromean` and `computesed`. The test program will first check if your implementation is correct and then measure how many cycles it takes.

**Known issue for computegp: the results are very small values and thus checking correctness is not accurate enough for now.**

### Work with the real t-SNE program

After you confirm that your kernel is indeed correct and more performant, you may want to test it with real t-SNE program in an end-to-end manner. To do this, first navigate to the `main.cpp`.

```cpp
/* main.cpp */

// comment out the old kernel
// using namespace computeGPv1;

using namespace computeGPv2; // provide your new implementation

/* The rest should remain untouched. */
```

To run the t-SNE program, do the following.

```shell
make tsne
./build/tsne
```

### Inspect your result visually

The result generated by the t-SNE program will be written to `output.txt`. Visualize the results by the following command.

```shell
python visualize.py
```

## Caveat and TODO

This summarizes a list of features unavailable for the current implementation.

1. Currently we only support 2-dimension output. This is hardcoded in the `parameter.hpp`.
2. We cannot time the t-SNE program for now, i.e. no end-to-end performance report. However, we can time the kernels individually.
3. We have not conducted a comprehensive study on choosing the most efficient compiler and the set of compiler flags.
